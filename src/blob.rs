//! Blob storage support for large data fields.
//!
//! This module provides types and traits for handling large binary data that needs
//! to be split into chunks for efficient storage and retrieval. Data larger than
//! 60KB is automatically chunked when stored.
//!
//! # Overview
//!
//! Blob fields in models are marked with the `#[netabase_blob_item]` attribute.
//! The macro generates implementations that handle:
//! - Automatic chunking of data > 60KB
//! - Serialization with postcard
//! - Reconstruction from chunks
//!
//! # Example
//!
//! ```rust,ignore
//! use netabase_macros::NetabaseBlobItem;
//! use serde::{Serialize, Deserialize};
//!
//! #[derive(NetabaseBlobItem, Serialize, Deserialize, Clone)]
//! pub struct LargeFile {
//!     pub data: Vec<u8>,
//!     pub metadata: String,
//! }
//! ```
//!
//! See [tests/blob_query_methods.rs](../tests/blob_query_methods.rs) for usage examples.

use serde::{Serialize, de::DeserializeOwned};

/// Link type for blob data, either complete or chunked.
///
/// This enum represents blob data in two states:
/// - `Complete`: The full blob item (used before storage)
/// - `Blobs`: A vec of blob chunks (used during storage/retrieval)
pub enum BlobLink<T: NetabaseBlobItem> {
    /// Complete blob item, not yet chunked
    Complete(T),
    /// Blob split into chunks for storage
    Blobs(Vec<T::Blobs>),
}

/// Trait for types that can be stored as blobs with automatic chunking.
///
/// Types implementing this trait can be automatically split into 60KB chunks
/// for storage and reconstructed on retrieval. The trait is typically derived
/// using the `#[netabase_blob_item]` macro.
///
/// # Chunk Size
///
/// Data is split into 60KB (60,000 byte) chunks to balance:
/// - Database page efficiency
/// - Memory usage during reads
/// - Parallel fetch granularity
///
/// # Serialization
///
/// Uses postcard for efficient binary serialization that's suitable for
/// network transmission in decentralized systems.
pub trait NetabaseBlobItem: Sized + Serialize + DeserializeOwned {
    /// The associated enum type for blob chunks.
    ///
    /// Generated by the macro, each variant represents a chunk.
    type Blobs;

    /// Wrap a chunk of data into the specific Blob enum variant.
    ///
    /// # Arguments
    /// * `index` - Chunk index (0-255)
    /// * `data` - Chunk data (up to 60KB)
    fn wrap_blob(index: u8, data: Vec<u8>) -> Self::Blobs;

    /// Extract the index and data from a Blob enum variant.
    ///
    /// Returns `None` if the blob type doesn't match.
    fn unwrap_blob(blob: &Self::Blobs) -> Option<(u8, Vec<u8>)>;

    /// Split this item into blob chunks.
    ///
    /// Serializes the item with postcard and splits into 60KB chunks.
    /// Each chunk is wrapped in the appropriate enum variant.
    ///
    /// # Returns
    /// A vector of blob chunks, empty if the item serializes to empty bytes.
    fn split_into_blobs(&self) -> Vec<Self::Blobs> {
        let serialized = postcard::to_allocvec(self).unwrap();

        if serialized.is_empty() {
            return Vec::new();
        }

        serialized
            .chunks(60000)
            .enumerate()
            .map(|(i, chunk)| Self::wrap_blob(i as u8, chunk.to_vec()))
            .collect()
    }

    /// Reconstruct the original item from blob chunks.
    ///
    /// Takes a vector of blob chunks, sorts them by index, concatenates
    /// the data, and deserializes back into the original item.
    ///
    /// # Arguments
    /// * `blobs` - Vector of blob chunks (order doesn't matter, will be sorted)
    ///
    /// # Returns
    /// The reconstructed item
    ///
    /// # Panics
    /// Panics if deserialization fails, which indicates corrupted data.
    fn reconstruct_from_blobs(blobs: Vec<Self::Blobs>) -> Self {
        if blobs.is_empty() {
            // Handle empty case: try to decode from empty bytes
            // This assumes that empty struct encodes to empty bytes and vice versa
            return postcard::from_bytes(&[]).unwrap();
        }

        let mut parts: Vec<(u8, Vec<u8>)> =
            blobs.iter().filter_map(|b| Self::unwrap_blob(b)).collect();
        parts.sort_by_key(|(i, _)| *i);
        let mut result = Vec::new();
        for (_, part) in parts {
            result.extend(part);
        }
        postcard::from_bytes(&result).unwrap()
    }
}
